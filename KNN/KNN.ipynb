{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "(12686, 784)\n",
      "(2115, 784)\n"
     ]
    }
   ],
   "source": [
    "#data\n",
    "mnist = input_data.read_data_sets(\"MNIST-data\", one_hot=False)\n",
    "x_, y_ = mnist.train.next_batch(60000)\n",
    "Y = []\n",
    "X = []\n",
    "for e in range(60000):\n",
    "    if y_[e]<=1:\n",
    "        Y.append(y_[e])\n",
    "        X.append(x_[e])\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)\n",
    "print(X.shape)\n",
    "        \n",
    "x_test,y_test = mnist.test.next_batch(10000)\n",
    "test_Y = []\n",
    "test_X = []\n",
    "for e in range(10000):\n",
    "    if y_test[e]<=1:\n",
    "        test_Y.append(y_test[e])\n",
    "        test_X.append(x_test[e])\n",
    "test_X = np.asarray(test_X)\n",
    "test_Y = np.asarray(test_Y)\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(p,t):\n",
    "    return np.sqrt((np.square(np.subtract(p,t))).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN has no training\n",
    "K = 15\n",
    "#metric = average\n",
    "\n",
    "labels = []\n",
    "for e in range(1):\n",
    "    #distance with each point in train set\n",
    "    distance = np.ones(len(X))\n",
    "    for j in range(len(X)):\n",
    "        distance[j] = dist(test_X[e],X[j])\n",
    "    ind = np.argpartition(distance, -K)[-K:]\n",
    "    count1 = 0\n",
    "    for i in ind:\n",
    "        if Y[i]==1:\n",
    "            count1+=1\n",
    "    if count1>=(K-count1):\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(labels[0])\n",
    "print(e)\n",
    "for i in ind:\n",
    "    print(Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(test_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(l,y)\n",
    "    acc = 0\n",
    "    for v in range(len(l)):\n",
    "        if l[v]==y[v]:\n",
    "            acc+=1\n",
    "    return acc/len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.16862746 0.16862746 0.09019608 0.02352941\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.31764707 0.57254905 0.57254905 0.57254905\n 0.9568628  0.95294124 0.77647066 0.62352943 0.57254905 0.57254905\n 0.08627451 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.35686275 0.8235295\n 0.9803922  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n 0.9921569  0.9921569  0.9921569  0.9921569  0.9725491  0.5686275\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.14117648 0.7568628  0.9843138  0.9921569  0.9921569  0.9921569\n 0.9921569  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n 0.9921569  0.9921569  0.9921569  0.9843138  0.5019608  0.02745098\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.5137255  0.9921569\n 0.9921569  0.9921569  0.9921569  0.8078432  0.45098042 0.45098042\n 0.6117647  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n 0.9921569  0.9921569  0.9921569  0.1764706  0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.37254903 0.9803922  0.9921569  0.9921569  0.9921569\n 0.9921569  0.09019608 0.         0.         0.01960784 0.1137255\n 0.36862746 0.05490196 0.09411766 0.65882355 0.9450981  0.9921569\n 0.9921569  0.8196079  0.13333334 0.         0.         0.\n 0.         0.         0.         0.         0.         0.8352942\n 0.9921569  0.9921569  0.9921569  0.9921569  0.5921569  0.01568628\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.40784317 0.9921569  0.9921569  0.9921569\n 0.82745105 0.         0.         0.         0.         0.\n 0.         0.         0.         1.         0.9921569  0.9921569\n 0.9921569  0.9921569  0.45098042 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.5529412  0.9921569  0.9921569  0.9921569  0.9921569  0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.9960785  0.9921569  0.9921569  0.9921569  0.9921569\n 0.45098042 0.         0.         0.         0.         0.\n 0.         0.         0.01960784 0.5764706  0.9803922  0.9921569\n 0.9921569  0.9921569  0.9921569  0.         0.         0.\n 0.         0.         0.         0.         0.         0.9960785\n 0.9921569  0.9921569  0.9921569  0.8862746  0.13725491 0.\n 0.         0.         0.         0.         0.         0.08235294\n 0.5137255  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n 0.5686275  0.         0.         0.         0.         0.\n 0.         0.         0.         0.9960785  0.9921569  0.9921569\n 0.9921569  0.83921576 0.         0.         0.         0.\n 0.         0.04705883 0.48627454 0.9333334  0.9921569  0.9921569\n 0.9921569  0.9921569  0.9921569  0.5921569  0.20000002 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.8313726  0.9921569  0.9921569  0.9921569  0.9725491\n 0.40000004 0.         0.20000002 0.27450982 0.27450982 0.7058824\n 0.9921569  0.9921569  0.9921569  0.9921569  0.9921569  0.81568635\n 0.46274513 0.02352941 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.13333334\n 0.81568635 0.9921569  0.9921569  0.9921569  0.8117648  0.6627451\n 0.9058824  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n 0.9921569  0.9921569  0.79215693 0.12156864 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.18431373 0.9921569\n 0.9921569  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n 0.9921569  0.9921569  0.9921569  0.9921569  0.9921569  0.7843138\n 0.08235294 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.02745098 0.14901961 0.8235295  0.9921569\n 0.9921569  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n 0.9921569  0.92549026 0.7137255  0.08235294 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.3529412  0.9607844  0.97647065 0.9921569\n 0.9921569  0.9921569  0.9921569  0.9921569  0.97647065 0.69803923\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.54509807 0.9921569  0.9921569  0.9921569\n 0.9921569  0.95294124 0.29411766 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.09803922 0.1764706  0.1764706  0.1764706  0.1764706  0.16078432\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-cf7fb7ab1aa3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msk_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0msk_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneigh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf1.12\\lib\\site-packages\\sklearn\\neighbors\\classification.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mdata\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \"\"\"\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf1.12\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    550\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.16862746 0.16862746 0.09019608 0.02352941\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.31764707 0.57254905 0.57254905 0.57254905\n 0.9568628  0.95294124 0.77647066 0.62352943 0.57254905 0.57254905\n 0.08627451 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.35686275 0.8235295\n 0.9803922  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n 0.9921569  0.9921569  0.9921569  0.9921569  0.9725491  0.5686275\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.14117648 0.7568628  0.9843138  0.9921569  0.9921569  0.9921569\n 0.9921569  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n 0.9921569  0.9921569  0.9921569  0.9843138  0.5019608  0.02745098\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.5137255  0.9921569\n 0.9921569  0.9921569  0.9921569  0.8078432  0.45098042 0.45098042\n 0.6117647  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n 0.9921569  0.9921569  0.9921569  0.1764706  0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.37254903 0.9803922  0.9921569  0.9921569  0.9921569\n 0.9921569  0.09019608 0.         0.         0.01960784 0.1137255\n 0.36862746 0.05490196 0.09411766 0.65882355 0.9450981  0.9921569\n 0.9921569  0.8196079  0.13333334 0.         0.         0.\n 0.         0.         0.         0.         0.         0.8352942\n 0.9921569  0.9921569  0.9921569  0.9921569  0.5921569  0.01568628\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.40784317 0.9921569  0.9921569  0.9921569\n 0.82745105 0.         0.         0.         0.         0.\n 0.         0.         0.         1.         0.9921569  0.9921569\n 0.9921569  0.9921569  0.45098042 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.5529412  0.9921569  0.9921569  0.9921569  0.9921569  0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.9960785  0.9921569  0.9921569  0.9921569  0.9921569\n 0.45098042 0.         0.         0.         0.         0.\n 0.         0.         0.01960784 0.5764706  0.9803922  0.9921569\n 0.9921569  0.9921569  0.9921569  0.         0.         0.\n 0.         0.         0.         0.         0.         0.9960785\n 0.9921569  0.9921569  0.9921569  0.8862746  0.13725491 0.\n 0.         0.         0.         0.         0.         0.08235294\n 0.5137255  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n 0.5686275  0.         0.         0.         0.         0.\n 0.         0.         0.         0.9960785  0.9921569  0.9921569\n 0.9921569  0.83921576 0.         0.         0.         0.\n 0.         0.04705883 0.48627454 0.9333334  0.9921569  0.9921569\n 0.9921569  0.9921569  0.9921569  0.5921569  0.20000002 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.8313726  0.9921569  0.9921569  0.9921569  0.9725491\n 0.40000004 0.         0.20000002 0.27450982 0.27450982 0.7058824\n 0.9921569  0.9921569  0.9921569  0.9921569  0.9921569  0.81568635\n 0.46274513 0.02352941 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.13333334\n 0.81568635 0.9921569  0.9921569  0.9921569  0.8117648  0.6627451\n 0.9058824  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n 0.9921569  0.9921569  0.79215693 0.12156864 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.18431373 0.9921569\n 0.9921569  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n 0.9921569  0.9921569  0.9921569  0.9921569  0.9921569  0.7843138\n 0.08235294 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.02745098 0.14901961 0.8235295  0.9921569\n 0.9921569  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n 0.9921569  0.92549026 0.7137255  0.08235294 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.3529412  0.9607844  0.97647065 0.9921569\n 0.9921569  0.9921569  0.9921569  0.9921569  0.97647065 0.69803923\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.54509807 0.9921569  0.9921569  0.9921569\n 0.9921569  0.95294124 0.29411766 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.09803922 0.1764706  0.1764706  0.1764706  0.1764706  0.16078432\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=15)\n",
    "neigh.fit(X, Y)\n",
    "sk_labels = []\n",
    "for e in test_X:\n",
    "    sk_labels.append(neigh.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sklearn KNN accuracy = \",accuracy(sk_labels,test_Y))\n",
    "print(\"My KNN accuracy = \",accuracy(labels,test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
