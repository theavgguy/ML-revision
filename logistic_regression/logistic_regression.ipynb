{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "(12648, 784)\n",
      "(2115, 784)\n"
     ]
    }
   ],
   "source": [
    "#data\n",
    "mnist = input_data.read_data_sets(\"MNIST-data\", one_hot=False)\n",
    "x_, y_ = mnist.train.next_batch(60000)\n",
    "Y = []\n",
    "X = []\n",
    "for e in range(60000):\n",
    "    if y_[e]<=1:\n",
    "        Y.append(y_[e])\n",
    "        X.append(x_[e])\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)\n",
    "print(X.shape)\n",
    "        \n",
    "x_test,y_test = mnist.test.next_batch(10000)\n",
    "test_Y = []\n",
    "test_X = []\n",
    "for e in range(10000):\n",
    "    if y_test[e]<=1:\n",
    "        test_Y.append(y_test[e])\n",
    "        test_X.append(x_test[e])\n",
    "test_X = np.asarray(test_X)\n",
    "test_Y = np.asarray(test_Y)\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12648\n"
     ]
    }
   ],
   "source": [
    "c1 = 0\n",
    "c0 = 0\n",
    "for l in Y:\n",
    "    if l==0:\n",
    "        c0 = c0+1\n",
    "    elif l==1:\n",
    "        c1 = c1+1\n",
    "    \n",
    "print(c1+c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12648, 785)\n"
     ]
    }
   ],
   "source": [
    "X = np.insert(X,len(X[0]),1,axis=1)\n",
    "test_X = np.insert(test_X,len(test_X[0]),1,axis=1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "dim = 28\n",
    "batch_size = 128\n",
    "lr = 0.0005\n",
    "iterations= 10000\n",
    "train_total = len(X)\n",
    "train_epochs = int(train_total/batch_size)\n",
    "test_total = len(test_X)\n",
    "test_epochs = int(test_total/batch_size)\n",
    "\n",
    "#variables\n",
    "W = np.ones((dim*dim+1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward prop\n",
    "def sigmoid(x):\n",
    "    val = 1.+np.exp(-x)\n",
    "    if val==0:\n",
    "        print(\"error\")\n",
    "    return 1./val\n",
    "\n",
    "def forward_prop(X,W):\n",
    "    logits = np.dot(X,W)\n",
    "    return sigmoid(logits)\n",
    "\n",
    "#loss function and gradients\n",
    "def BCE_loss(activations,labels):\n",
    "    cost1 = labels*np.log(activations)\n",
    "    cost2 = (1-labels)*np.log(1-activations)\n",
    "    cost = cost1+cost2\n",
    "    print(len(activations))\n",
    "    cost = cost.sum()/len(activations)\n",
    "    cost = -1*cost\n",
    "    return cost\n",
    "\n",
    "def gradient(p,t,X):\n",
    "    error = p-t\n",
    "    gradients = np.dot(error.T,X)\n",
    "    print(len(p))\n",
    "    gradients = gradients/len(p)\n",
    "    gradients = gradients*lr\n",
    "    return gradients\n",
    "\n",
    "def update_rule(G,W):\n",
    "    W = W - G.T\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12648\n",
      "12648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragha\\Anaconda3\\envs\\tf1.12\\lib\\site-packages\\ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\ragha\\Anaconda3\\envs\\tf1.12\\lib\\site-packages\\ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in multiply\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12648\n",
      "0 nan\n",
      "12648\n",
      "12648\n",
      "12648\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-82830589f1e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#    W = update_rule(g,W)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#    loss = loss+BCE_loss(predictions,labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-106-0f2fa47a463f>\u001b[0m in \u001b[0;36mforward_prop\u001b[1;34m(X, W)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "itx = []\n",
    "for i in range(iterations):\n",
    "    loss = 0\n",
    "    for j in range(train_epochs):\n",
    "        train = X[batch_size*j:batch_size*(j+1)]\n",
    "        labels = Y[batch_size*j:batch_size*(j+1)]\n",
    "        predictions = forward_prop(train,W)\n",
    "        g = gradient(predictions,labels,train)\n",
    "        W = update_rule(g,W)\n",
    "        loss = loss+BCE_loss(predictions,labels)\n",
    "    losses.append(loss/batch_size)\n",
    "    itx.append(i)\n",
    "    if i%100==0:\n",
    "        print(i, losses[-1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt(itx,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragha\\Anaconda3\\envs\\tf1.12\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit score:  0.9995271867612293\n"
     ]
    }
   ],
   "source": [
    "# Scikit Logistic Regression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X,Y)\n",
    "\n",
    "#Score is Mean Accuracy\n",
    "scikit_score = clf.score(test_X,test_Y)\n",
    "print ('Scikit score: ', scikit_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
